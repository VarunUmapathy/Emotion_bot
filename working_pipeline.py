import speech_recognition as sr
import os
from openai import OpenAI
from datetime import datetime
from elevenlabs.client import ElevenLabs
import subprocess
import time

# API Keys (use environment variables for security)
# IMPORTANT: Replace with your actual API keys
OPENAI_KEY = os.environ.get("OPENAI_API_KEY", "YOUR_OPENAI_API_KEY")
ELEVENLABS_API_KEY = "sk_1b290621776c4f8722821defc4fcafb0187422a93a4a522d"

WHISPER_MODEL = "whisper-1"
GPT_MODEL = "gpt-4o"
ELEVENLABS_VOICE_ID = "C2RGMrNBTZaNfddRPeRH"
ELEVENLABS_MODEL_ID = "eleven_multilingual_v2"

# Ensure API keys are set before proceeding
if not OPENAI_KEY or OPENAI_KEY == "YOUR_OPENAI_API_KEY":
    raise ValueError("OPENAI_API_KEY environment variable not set or is a placeholder.")
if not ELEVENLABS_API_KEY or ELEVENLABS_API_KEY == "YOUR_ELEVENLABS_API_KEY":
    raise ValueError("ELEVENLABS_API_KEY environment variable not set or is a placeholder.")

openai_client = OpenAI(api_key=OPENAI_KEY)
elevenlabs_client = ElevenLabs(api_key=ELEVENLABS_API_KEY)

# --- MODIFIED: SYSTEM PROMPT for Intelligent Stopping ---
system_rules = """
роирпА роТро░рпБ роЙрогро░рпНроЪро┐ро╡роЪрокрпНрокроЯрпНроЯ, ро╡ро╛ро┤рпНро╡ро┐ропро▓рпН родрооро┐ро┤рпН рокрпЗроЪрпБроорпН роирокро░рпН рооро╛родро┐ро░ро┐ рокродро┐ро▓рпН роЪрпКро▓рпНро▓ ро╡рпЗрогрпНроЯрпБроорпН.

ро╡ро┐родро┐роХро│рпН:
- рокродро┐ро▓рпН 1тАУ2 ро╡ро░ро┐роХро│ро┐ро▓рпН роороЯрпНроЯрпБроорпН.
- роЗро▓роХрпНроХро┐роп родрооро┐ро┤рпН ро╡рпЗрогрпНроЯро╛роорпН; роЪро┐роорпНрокро┐ро│ро╛ рокрпЗроЪрпБ.
- роЪрпЛроХроорпН, роЪроирпНродрпЛро╖роорпН, роХрпЛрокроорпН, родройро┐роорпИтАФроОродрпБ ро╡роирпНродро╛ро▓рпБроорпН роЙрогро░рпНроЪрпНроЪро┐ родрпЖро░ро┐роЮрпНроЪ рооро╛родро┐ро░ро┐ рокрпЗроЪрпБ.
- роирогрпНрокройро╛роХ рокрпЗроЪрпБ, роЖро▒рпБродро▓рпН родрпЗро╡рпИрокрпНрокроЯрпНроЯро╛ роорпЖродрпБро╡ро╛ рокрпЗроЪрпБ.
- роЙроирпНродройрпН рокродро┐ро▓рпН ро╡ро╛ро┤рпНроХрпНроХрпИропрпЛроЯрпБ роЪроорпНрокроирпНродрокрпНрокроЯрпНроЯ роЙрогро░рпНро╡рпБроХро│рпИ роХрпКрогрпНроЯрпБ роЗро░рпБроХрпНроХроЯрпНроЯрпБроорпН.
- рокропройро░рпН роЙро░рпИропро╛роЯро▓рпИ роорпБроЯро┐роХрпНроХ ро╡ро┐ро░рпБроорпНрокро┐ройро╛ро▓рпН, "bye", "see you", "роЕрокрпНрокрпБро▒роорпН рокро╛роХрпНроХро▓ро╛роорпН", "рокрпЛропрпНроЯрпНроЯрпБ ро╡ро░рпЗройрпН" рокрпЛройрпНро▒ ро╡ро╛ро░рпНродрпНродрпИроХро│рпИ роЪрпКройрпНройро╛ро▓рпН,
  роирпАропрпБроорпН роТро░рпБ роЪрпБро░рпБроХрпНроХрооро╛рой, роЕройрпНрокро╛рой рокро┐ро░ро┐ро╡рпБроХрпНроХрпБро░ро┐роп рокродро┐ро▓рпИ роХрпКроЯрпБродрпНродрпБро╡ро┐роЯрпНроЯрпБ, роЕродройрпН роЗро▒рпБродро┐ропро┐ро▓рпН [END_CONVERSATION] роОройрпНро▒ родройро┐ роХрпБро▒ро┐ропрпАроЯрпНроЯрпИроЪрпН роЪрпЗро░рпНроХрпНроХ ро╡рпЗрогрпНроЯрпБроорпН.
"""
# --- REMOVED: Farewell phrases list is no longer needed ---
# farewell_phrases = ["роЕрокрпНрокрпБро▒роорпН рокро╛роХрпНроХро▓ро╛роорпН", "рокро╛ро░рпНроХрпНроХро▓ро╛роорпН", "рокрпЛропрпНроЯрпНроЯрпБ ро╡ро░рпЗройрпН", "роЪро░ро┐ рокро╛ропрпН", "bye", "роЕрокрпНрокрпБро▒роорпН рокрпЗроЪрпБро╡рпЛроорпН"]

def transcribe_with_whisper(audio_data):
    """Transcribes audio data using OpenAI's Whisper API."""
    try:
        response = openai_client.audio.transcriptions.create(
            model=WHISPER_MODEL,
            file=audio_data,
            language="ta",
            response_format="text"
        )
        return response.strip()
    except Exception as e:
        print(f"Error during Whisper transcription: {e}")
        return ""

def generate_response_with_gpt(messages_list):
    """Generates a conversational response using GPT-4o with a full message history."""
    try:
        chat_response = openai_client.chat.completions.create(
            model=GPT_MODEL,
            messages=messages_list,
            temperature=0.7,
        )
        return chat_response.choices[0].message.content.strip()
    except Exception as e:
        print(f"Error generating GPT response: {e}")
        return "рооройрпНройро┐роХрпНроХро╡рпБроорпН, роТро░рпБ рокро┐ро┤рпИ роПро▒рпНрокроЯрпНроЯродрпБ. роорпАрогрпНроЯрпБроорпН роорпБропро▒рпНроЪро┐роХрпНроХро╡рпБроорпН."

def synthesize_speech_elevenlabs(text, output_path="elevenlabs_response.mp3"):
    """Synthesizes text to speech using ElevenLabs and plays it."""
    print("ЁЯФК Converting response to audio using ElevenLabs...")
    try:
        # --- MODIFIED: Removed the stream to file logic ---
        # The new client library can directly stream to a player.
        
        audio_generator = elevenlabs_client.text_to_speech.convert(
            text=text,
            voice_id=ELEVENLABS_VOICE_ID,
            model_id=ELEVENLABS_MODEL_ID,
            output_format="mp3_44100_128",
        )

        with open(output_path, "wb") as f:
            for chunk in audio_generator:
                f.write(chunk)
        
        print(f"тЬЕ Audio saved as: {output_path}")
        print("тЦ╢я╕П Playing audio on laptop speakers...")
        
        ffplay_path = r"C:\ffmpeg\ffmpeg-7.1.1-essentials_build\bin\ffplay.exe"
        subprocess.run([ffplay_path, "-autoexit", output_path])
        
        print("Playback completed.")
        return output_path
    except Exception as e:
        print(f"Error during ElevenLabs TTS synthesis or playback: {e}")
        return None

def main_conversation_loop():
    """Main function to run the conversational agent."""
    print(f"[{datetime.now().strftime('%H:%M:%S')}] Starting conversational agent...\n")
    
    # Initialize the conversation with the system rules. This list is the "memory".
    messages = [{"role": "system", "content": system_rules}]
    
    recognizer = sr.Recognizer()
    
    with sr.Microphone(device_index=1) as source:
        # Adjust for ambient noise only once at the beginning
        print("Adjusting for ambient noise... Please wait 5 seconds.")
        recognizer.adjust_for_ambient_noise(source, duration=5)
        print("Adjustment complete. You can start talking.")
        
        while True:
            try:
                print("\nListening for your voice...")
                # --- MODIFIED: Dynamic Listening logic ---
                # This will stop listening after 3 seconds of silence.
                # The phrase_time_limit handles the silence timeout.
                audio = recognizer.listen(source, phrase_time_limit=7)
                print("Listening stopped.")

                # Save audio temporarily
                temp_audio_file = "temp_audio.wav"
                with open(temp_audio_file, "wb") as f:
                    f.write(audio.get_wav_data())

                # Transcribe the user's speech
                with open(temp_audio_file, "rb") as audio_file:
                    transcription = transcribe_with_whisper(audio_file)
                
                if not transcription:
                    print("No speech detected or transcription failed. Listening again...")
                    continue
                
                print(f"ЁЯУЭ You said: {transcription}")
                
                # --- REMOVED: Hardcoded farewell check is no longer needed ---
                # if any(phrase in transcription.lower() for phrase in farewell_phrases):
                #     print("Goodbye detected. Ending conversation.")
                #     break
                
                # Add the user's message to the conversation history
                messages.append({"role": "user", "content": transcription})
                
                # Generate a response from GPT using the full conversation history
                gpt_response = generate_response_with_gpt(messages)
                
                # --- MODIFIED: Check for the special token in GPT's response ---
                if "[END_CONVERSATION]" in gpt_response:
                    print("Goodbye detected. Ending conversation.")
                    # Clean up the response for speaking
                    final_response = gpt_response.replace("[END_CONVERSATION]", "").strip()
                    print(f"\nЁЯдЦ Agent's Response: {final_response}")
                    synthesize_speech_elevenlabs(final_response)
                    break
                else:
                    print(f"\nЁЯдЦ Agent's Response: {gpt_response}")
                    # Add the agent's response to the conversation history
                    messages.append({"role": "assistant", "content": gpt_response})
                    
                    # Synthesize and play the agent's response
                    synthesize_speech_elevenlabs(gpt_response)
            
            except sr.WaitTimeoutError:
                print("Timeout: No speech detected for 3 seconds. Listening again...")
                continue
            except Exception as e:
                print(f"An unexpected error occurred: {e}")
                time.sleep(1) # Wait a bit before retrying

if __name__ == "__main__":
    print("\n--- Listing available voices in your ElevenLabs account ---")
    try:
        voices = elevenlabs_client.voices.get_all()
        if voices.voices:
            print("Available Voices:")
            for voice in voices.voices:
                print(f"  Voice ID: {voice.voice_id}, Name: {voice.name}")
        else:
            print("No voices found. Check your API key and plan.")
    except Exception as e:
        print(f"Could not list voices. Error: {e}")

    main_conversation_loop()
    print("Conversation ended. Thank you for using the agent!")